<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Wendy's Corner (Posts about AI)</title><link>https://www.pengyin-shan.com/</link><description></description><atom:link href="https://www.pengyin-shan.com/categories/cat_ai.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2019 &lt;a href="mailto:pengyin.shan@outlook.com"&gt;Pengyin(Wendy) Shan&lt;/a&gt; </copyright><lastBuildDate>Sun, 19 May 2019 12:26:29 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Coursera Machine Learning Notes</title><link>https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/</link><dc:creator>Pengyin(Wendy) Shan</dc:creator><description>&lt;div&gt;&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#week-1"&gt;Week 1&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#supervised-learning"&gt;Supervised Learning&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#example-1"&gt;Example 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#example-2"&gt;Example 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#unsupervised-learning"&gt;Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#model-representation"&gt;Model Representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#cost-function"&gt;Cost Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#gradient-descent"&gt;Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#linear-algebra-review"&gt;Linear Algebra Review&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#manipulation"&gt;Manipulation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#week2-linear-regression"&gt;Week2: Linear Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#multivariate-linear-regression"&gt;Multivariate Linear Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#gradient-descent-for-multiple-variables"&gt;Gradient Descent For Multiple Variables&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#feature-scaling"&gt;Feature Scaling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#debug-gd"&gt;Debug GD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#features-and-polynomial-regression"&gt;Features and Polynomial Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#normal-equation"&gt;Normal Equation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#noninvertability"&gt;Noninvertability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#week3"&gt;Week3&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#logistic-regression"&gt;Logistic Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#logisticsigmoid-function"&gt;Logistic/Sigmoid Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#decision-boundary"&gt;Decision Boundary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#cost-function-for-logistic-regression"&gt;Cost Function for Logistic Regression&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#simplified-cost-function-gradient-descent"&gt;Simplified Cost Function &amp;amp; Gradient Descent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#advanced-optimization"&gt;Advanced Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#one-vs-all"&gt;One-vs-All&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#overfitting"&gt;Overfitting&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#cost-function_1"&gt;Cost Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#regularized-linear-regression"&gt;Regularized Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#regularized-logistic-regression"&gt;Regularized Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#week4"&gt;Week4&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#neural-networks"&gt;Neural Networks&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#model-representation_1"&gt;Model Representation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#intuitions"&gt;Intuitions&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#muti-class-classification"&gt;Muti-class Classification&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#week5"&gt;Week5&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#cost-function-back-propagation"&gt;Cost Function &amp;amp; Back Propagation&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#cost-function_2"&gt;Cost Function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#backpropagation-algorithm"&gt;Backpropagation Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#backpropagation-intuition"&gt;Backpropagation Intuition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#unrolling-parameter"&gt;Unrolling Parameter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#gradient-checking"&gt;Gradient Checking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#random-initialization"&gt;Random Initialization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/#train-the-nn"&gt;Train the NN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="week-1"&gt;Week 1&lt;/h2&gt;
&lt;p&gt;Two definitions of Machine Learning are offered. Arthur Samuel described it as: &lt;code&gt;the field of study that gives computers the ability to learn without being explicitly programmed." This is an older, informal definition.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Tom Mitchell provides a more modern definition: &lt;code&gt;A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Example: playing checkers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;E = the experience of playing many games of checkers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;T = the task of playing checkers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;P = the probability that the program will win the next game.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning and Unsupervised learning.&lt;/p&gt;
&lt;h3 id="supervised-learning"&gt;Supervised Learning&lt;/h3&gt;
&lt;p&gt;In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.&lt;/p&gt;
&lt;p&gt;Supervised learning problems are categorized into &lt;code&gt;regression&lt;/code&gt; and &lt;code&gt;classification&lt;/code&gt; problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.&lt;/p&gt;
&lt;h4 id="example-1"&gt;Example 1&lt;/h4&gt;
&lt;p&gt;Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem.&lt;/p&gt;
&lt;p&gt;We could turn this example into a classification problem by instead making our output about whether the house "sells for more or less than the asking price." Here we are classifying the houses based on price into two discrete categories.&lt;/p&gt;
&lt;h4 id="example-2"&gt;Example 2&lt;/h4&gt;
&lt;p&gt;(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture&lt;/p&gt;
&lt;p&gt;(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign.&lt;/p&gt;
&lt;h3 id="unsupervised-learning"&gt;Unsupervised Learning&lt;/h3&gt;
&lt;p&gt;Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.&lt;/p&gt;
&lt;p&gt;We can derive this structure by &lt;code&gt;clustering&lt;/code&gt; the data based on relationships among the variables in the data.&lt;/p&gt;
&lt;p&gt;With unsupervised learning there is no feedback based on the prediction results.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;p&gt;Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on.&lt;/p&gt;
&lt;p&gt;Non-clustering: The "Cocktail Party Algorithm", allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party).&lt;/p&gt;
&lt;h3 id="model-representation"&gt;Model Representation&lt;/h3&gt;
&lt;p&gt;To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function &lt;code&gt;h : X â Y&lt;/code&gt; so that &lt;code&gt;h(x)&lt;/code&gt; is a âgoodâ predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. &lt;/p&gt;
&lt;p&gt;When the target variable that weâre trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.&lt;/p&gt;
&lt;h3 id="cost-function"&gt;Cost Function&lt;/h3&gt;
&lt;p&gt;We can measure the &lt;code&gt;accuracy&lt;/code&gt; of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/cost_function.png"&gt;&lt;/p&gt;
&lt;p&gt;This function is otherwise called the "Squared error function", or "Mean squared error". The mean is halved &lt;code&gt;1/2&lt;/code&gt; as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the &lt;code&gt;1/2&lt;/code&gt; term.&lt;/p&gt;
&lt;p&gt;If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line which passes through these scattered data points.&lt;/p&gt;
&lt;p&gt;Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of J will be 0. &lt;/p&gt;
&lt;p&gt;When \theta_1 =1, we get a slope of 1 which goes through every single data point in our model. Conversely, when \theta_1 =0.5, we see the vertical distance from our fit to the data points increase.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/cost_function_2.png"&gt;&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;contour plot&lt;/code&gt; is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/cost_function_3.png"&gt;&lt;/p&gt;
&lt;h3 id="gradient-descent"&gt;Gradient Descent&lt;/h3&gt;
&lt;p&gt;So we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That's where gradient descent comes in.&lt;/p&gt;
&lt;p&gt;The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/g_d.png"&gt;&lt;/p&gt;
&lt;p&gt;We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph.&lt;/p&gt;
&lt;p&gt;The way we do this is by taking the derivative (the tangential line to a function) of our cost function. &lt;strong&gt;The slope of the tangent is the derivative at that point and it will give us a direction to move towards&lt;/strong&gt;. We make steps down the cost function in the direction with the steepest descent. The &lt;strong&gt;size&lt;/strong&gt; of each step is determined by the parameter &lt;code&gt;Î±&lt;/code&gt;, which is called the &lt;code&gt;learning rate&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/g_d_2.png"&gt;&lt;/p&gt;
&lt;p&gt;On a side note, we should adjust our parameter \alphaÎ± to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/g_d_3.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/g_d_4.png"&gt;&lt;/p&gt;
&lt;p&gt;The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.&lt;/p&gt;
&lt;p&gt;So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called &lt;code&gt;batch gradient descent&lt;/code&gt;. Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate Î± is not too large) to the global minimum. Indeed, J is a &lt;code&gt;convex quadratic function&lt;/code&gt;. &lt;/p&gt;
&lt;h3 id="linear-algebra-review"&gt;Linear Algebra Review&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Matrices&lt;/code&gt; are 2-dimensional arrays. A &lt;code&gt;vector&lt;/code&gt; is a matrix with one column and many rows, so vectors are a subset of matrices.&lt;/p&gt;
&lt;p&gt;To add or subtract two matrices, their dimensions must be the same.&lt;/p&gt;
&lt;h4 id="manipulation"&gt;Manipulation&lt;/h4&gt;
&lt;p&gt;We map the column of the vector onto each row of the matrix, multiplying each element and summing the result.&lt;/p&gt;
&lt;p&gt;The result is a vector. The number of columns of the matrix must equal the number of rows of the vector.&lt;/p&gt;
&lt;p&gt;An &lt;code&gt;m x n&lt;/code&gt; matrix multiplied by an &lt;code&gt;n x 1&lt;/code&gt; vector results in an &lt;code&gt;m x 1&lt;/code&gt; vector.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_v.png"&gt;&lt;/p&gt;
&lt;p&gt;An m x n matrix multiplied by an n x o matrix results in an m x o matrix.&lt;/p&gt;
&lt;p&gt;To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix.&lt;/p&gt;
&lt;p&gt;Matrices are not commutative: &lt;code&gt;AâBâ BâA&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Matrices are associative: &lt;code&gt;(AâB)âC=Aâ(BâC)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;identity matrix&lt;/code&gt;, when multiplied by any matrix of the same dimensions, results in the original matrix. It's just like multiplying numbers by 1. The identity matrix simply has 1's on the diagonal (upper left to lower right diagonal) and 0's elsewhere.&lt;/p&gt;
&lt;p&gt;When multiplying the identity matrix after some matrix (AâI), the square identity matrix's dimension should match the other matrix's columns. When multiplying the identity matrix before some other matrix (IâA), the square identity matrix's dimension should match the other matrix's rows.&lt;/p&gt;
&lt;p&gt;Multiplying by the &lt;code&gt;inverse&lt;/code&gt; results in the identity matrix.&lt;/p&gt;
&lt;p&gt;Matrices that don't have an inverse are &lt;code&gt;singular&lt;/code&gt; or degenerate.&lt;/p&gt;
&lt;p&gt;The transposition of a matrix is like rotating the matrix 90Â° in clockwise direction and then reversing it.&lt;/p&gt;
&lt;h2 id="week2-linear-regression"&gt;Week2: Linear Regression&lt;/h2&gt;
&lt;h3 id="multivariate-linear-regression"&gt;Multivariate Linear Regression&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r.png"&gt;&lt;/p&gt;
&lt;p&gt;The multivariable form of the hypothesis function accommodating these multiple features is as follows:&lt;/p&gt;
&lt;pre class="code literal-block"&gt;&lt;span&gt;&lt;/span&gt;hÎ¸(x)=Î¸0+Î¸1x1+Î¸2x2+Î¸3x3+â¯+Î¸nxn
&lt;/pre&gt;


&lt;p&gt;Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_2.png"&gt;&lt;/p&gt;
&lt;h4 id="gradient-descent-for-multiple-variables"&gt;Gradient Descent For Multiple Variables&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_3.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_4.png"&gt;&lt;/p&gt;
&lt;h4 id="feature-scaling"&gt;Feature Scaling&lt;/h4&gt;
&lt;p&gt;We can speed up gradient descent by having each of our input values in roughly the same range. This is because Î¸ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.&lt;/p&gt;
&lt;p&gt;The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_5.png"&gt;&lt;/p&gt;
&lt;p&gt;Two techniques to help with this are &lt;code&gt;feature scaling&lt;/code&gt; and &lt;code&gt;mean normalization&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. &lt;/p&gt;
&lt;p&gt;Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_6.png"&gt;&lt;/p&gt;
&lt;p&gt;Where Î¼i is the average of all the values for feature (i) and s is the range of values &lt;code&gt;max - min&lt;/code&gt;, or s is the standard deviation.&lt;/p&gt;
&lt;h4 id="debug-gd"&gt;Debug GD&lt;/h4&gt;
&lt;p&gt;Debugging gradient descent. Make a &lt;strong&gt;plot&lt;/strong&gt; with number of iterations on the x-axis. Now plot the cost function, &lt;code&gt;J(Î¸)&lt;/code&gt; over the number of iterations of gradient descent. If J(Î¸) ever increases, then you probably need to decrease Î±.&lt;/p&gt;
&lt;p&gt;Automatic convergence test. Declare convergence if J(Î¸) decreases by less than E in one iteration, where E is some small value such as &lt;code&gt;10â3&lt;/code&gt;. However in practice it's difficult to choose this threshold value.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_7.png"&gt;&lt;/p&gt;
&lt;h4 id="features-and-polynomial-regression"&gt;Features and Polynomial Regression&lt;/h4&gt;
&lt;p&gt;We can combine multiple features into one. For example, we can combine x1 and x2  into a new feature x3ã&lt;/p&gt;
&lt;p&gt;Our hypothesis function need not be linear (a straight line) if that does not fit the data well.&lt;/p&gt;
&lt;p&gt;We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).&lt;/p&gt;
&lt;p&gt;One important thing to keep in mind is, if you choose your features this way then &lt;strong&gt;feature scaling&lt;/strong&gt; becomes very important.&lt;/p&gt;
&lt;p&gt;eg. if &lt;code&gt;x1&lt;/code&gt; has range 1 - 1000 then range of &lt;code&gt;x2&lt;/code&gt; becomes 1 - 1000000 and that of &lt;code&gt;x3&lt;/code&gt; becomes 1 - 1000000000&lt;/p&gt;
&lt;h3 id="normal-equation"&gt;Normal Equation&lt;/h3&gt;
&lt;p&gt;In the "Normal Equation" method, we will minimize J by explicitly taking its derivatives with respect to the Î¸j âs, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/m_l_r_8.png"&gt;&lt;/p&gt;
&lt;h4 id="noninvertability"&gt;Noninvertability&lt;/h4&gt;
&lt;p&gt;When implementing the normal equation in octave we want to use the &lt;code&gt;pinv&lt;/code&gt; function rather than &lt;code&gt;inv.&lt;/code&gt; The &lt;code&gt;pinv&lt;/code&gt; function will give you a value of \thetaÎ¸ even if &lt;code&gt;x transpose * x&lt;/code&gt; is noninvertible. The common causes might be having :&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Redundant features, where two features are very closely related (i.e. they are linearly dependent)&lt;/li&gt;
&lt;li&gt;Too many features (e.g. &lt;code&gt;m â¤ n&lt;/code&gt;). In this case, delete some features or use &lt;code&gt;regularization&lt;/code&gt;.
Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="week3"&gt;Week3&lt;/h2&gt;
&lt;h3 id="logistic-regression"&gt;Logistic Regression&lt;/h3&gt;
&lt;p&gt;The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of &lt;strong&gt;discrete&lt;/strong&gt; values. For now, we will focus on the binary classification problem in which y can take on only two values, 0 and 1. For instance, if we are trying to build a spam classifier for email, then &lt;code&gt;x^i&lt;/code&gt; may be some features of a piece of email, and y may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, &lt;code&gt;yâ{0,1}&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;0 is also called the &lt;code&gt;negative class&lt;/code&gt;, and 1 the &lt;code&gt;positive class&lt;/code&gt;, and they are sometimes also denoted by the symbols &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;+&lt;/code&gt;. Given &lt;code&gt;x^(i)&lt;/code&gt;, the corresponding &lt;code&gt;y^(i)&lt;/code&gt; is also called the &lt;code&gt;label&lt;/code&gt; for the training example.&lt;/p&gt;
&lt;h4 id="logisticsigmoid-function"&gt;Logistic/Sigmoid Function&lt;/h4&gt;
&lt;p&gt;it also doesnât make sense for h(x) to take values larger than 1 or smaller than 0 when we know that y â {0, 1}. To fix this, letâs change the form for our hypotheses h(x) to satisfy &lt;code&gt;0â¤h(x)â¤1&lt;/code&gt;. This is accomplished by plugging &lt;code&gt;Î¸Tx&lt;/code&gt; into the Logistic Function.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_1.png"&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;code&gt;g(z)&lt;/code&gt;, shown here, maps any real number to the &lt;code&gt;(0, 1)&lt;/code&gt; interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;hÎ¸(x)&lt;/code&gt; will give us the probability that our output is &lt;code&gt;1&lt;/code&gt;. For example, hÎ¸(x)=0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the &lt;strong&gt;complement&lt;/strong&gt; of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).&lt;/p&gt;
&lt;h4 id="decision-boundary"&gt;Decision Boundary&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_2.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_3.png"&gt;&lt;/p&gt;
&lt;h4 id="cost-function-for-logistic-regression"&gt;Cost Function for Logistic Regression&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_4.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_5.png"&gt;&lt;/p&gt;
&lt;p&gt;If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity.&lt;/p&gt;
&lt;p&gt;If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity.&lt;/p&gt;
&lt;p&gt;Writing the cost function in this way &lt;strong&gt;guarantees&lt;/strong&gt; that &lt;code&gt;J(Î¸)&lt;/code&gt; is convex for logistic regression.&lt;/p&gt;
&lt;h5 id="simplified-cost-function-gradient-descent"&gt;Simplified Cost Function &amp;amp; Gradient Descent&lt;/h5&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_6.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_7.png"&gt;&lt;/p&gt;
&lt;h4 id="advanced-optimization"&gt;Advanced Optimization&lt;/h4&gt;
&lt;p&gt;Sophisticated algorithms: &lt;code&gt;Conjugate gradient&lt;/code&gt;, &lt;code&gt;BFGS&lt;/code&gt;, and &lt;code&gt;L-BFGS&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_8.png"&gt;&lt;/p&gt;
&lt;h4 id="one-vs-all"&gt;One-vs-All&lt;/h4&gt;
&lt;p&gt;Since y = {0,1...n}, we divide our problem into &lt;code&gt;n+1&lt;/code&gt; (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that 'y' is a member of one of our classes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_9.png"&gt;&lt;/p&gt;
&lt;h3 id="overfitting"&gt;Overfitting&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Underfitting&lt;/code&gt;, or &lt;code&gt;high bias&lt;/code&gt;, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, &lt;code&gt;overfitting&lt;/code&gt;, or &lt;code&gt;high variance&lt;/code&gt;, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.&lt;/p&gt;
&lt;p&gt;This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Reduce the number of features: Manually select which features to keep, or use a &lt;strong&gt;model selection algorithm&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Regularization&lt;/code&gt;: keep all the features, but reduce the magnitude of parameters &lt;code&gt;j(Î¸)&lt;/code&gt;. Regularization works well when we have a lot of slightly useful features.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="cost-function_1"&gt;Cost Function&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_10.png"&gt;&lt;/p&gt;
&lt;h4 id="regularized-linear-regression"&gt;Regularized Linear Regression&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_11.png"&gt;&lt;/p&gt;
&lt;h4 id="regularized-logistic-regression"&gt;Regularized Logistic Regression&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/lr_12.png"&gt;&lt;/p&gt;
&lt;h2 id="week4"&gt;Week4&lt;/h2&gt;
&lt;h3 id="neural-networks"&gt;Neural Networks&lt;/h3&gt;
&lt;h4 id="model-representation_1"&gt;Model Representation&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_1.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_2.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_3.png"&gt;&lt;/p&gt;
&lt;h3 id="intuitions"&gt;Intuitions&lt;/h3&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_4.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_5.png"&gt;&lt;/p&gt;
&lt;h4 id="muti-class-classification"&gt;Muti-class Classification&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/nn_6.png"&gt;&lt;/p&gt;
&lt;h2 id="week5"&gt;Week5&lt;/h2&gt;
&lt;h3 id="cost-function-back-propagation"&gt;Cost Function &amp;amp; Back Propagation&lt;/h3&gt;
&lt;h4 id="cost-function_2"&gt;Cost Function&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/cf_1.png"&gt;&lt;/p&gt;
&lt;h4 id="backpropagation-algorithm"&gt;Backpropagation Algorithm&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_1.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_2.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_3.png"&gt;&lt;/p&gt;
&lt;h4 id="backpropagation-intuition"&gt;Backpropagation Intuition&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_4.png"&gt;
&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_5.png"&gt;&lt;/p&gt;
&lt;h4 id="unrolling-parameter"&gt;Unrolling Parameter&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_6.png"&gt;&lt;/p&gt;
&lt;h4 id="gradient-checking"&gt;Gradient Checking&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_7.png"&gt;&lt;/p&gt;
&lt;h4 id="random-initialization"&gt;Random Initialization&lt;/h4&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_8.png"&gt;&lt;/p&gt;
&lt;h4 id="train-the-nn"&gt;Train the NN&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;pick a network architecture&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have: &lt;code&gt;Number of input units = dimension of features x^(i)&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of output units = number of classes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units) &lt;em&gt;Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="" src="https://www.pengyin-shan.com/images/2019/AI/bp_9.png"&gt;&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><guid>https://www.pengyin-shan.com/posts/2019/AI/coursera-machine-learning-notes/</guid><pubDate>Mon, 29 Apr 2019 04:00:00 GMT</pubDate></item><item><title>Study Note of Deep Learning From Scratch, written by æè¤åº·æ¯(Japan)</title><link>https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/</link><dc:creator>Pengyin(Wendy) Shan</dc:creator><description>&lt;div&gt;&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/#chapter-2-3-perceptron-neural-network-basics"&gt;Chapter 2-3: Perceptron + Neural Network Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/#chapter-4-6-study-of-neural-network"&gt;Chapter 4-6: Study of Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/#chapter-7-cnn"&gt;Chapter 7: CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/#chapter-8-application-of-deep-learning"&gt;Chapter 8: Application of Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/#network-in-network-nin"&gt;Network in Network ï¼NiN)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="chapter-2-3-perceptron-neural-network-basics"&gt;Chapter 2-3: Perceptron + Neural Network Basics&lt;/h2&gt;
&lt;p&gt;&lt;img alt="ai1" src="https://www.pengyin-shan.com/images/2018/ai/Chapter2-3-1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai2" src="https://www.pengyin-shan.com/images/2018/ai/Chapter2-3-2.jpg"&gt;
&lt;/p&gt;&lt;hr&gt;
&lt;h2 id="chapter-4-6-study-of-neural-network"&gt;Chapter 4-6: Study of Neural Network&lt;/h2&gt;
&lt;p&gt;&lt;img alt="ai3" src="https://www.pengyin-shan.com/images/2018/ai/Chapter4Note1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai4" src="https://www.pengyin-shan.com/images/2018/ai/Chapter5Note1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai5" src="https://www.pengyin-shan.com/images/2018/ai/Chapter5Note2.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai6" src="https://www.pengyin-shan.com/images/2018/ai/Chapter6Note1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai7" src="https://www.pengyin-shan.com/images/2018/ai/Chapter6Note2.jpg"&gt;
&lt;/p&gt;&lt;hr&gt;
&lt;h2 id="chapter-7-cnn"&gt;Chapter 7: CNN&lt;/h2&gt;
&lt;p&gt;&lt;img alt="ai6" src="https://www.pengyin-shan.com/images/2018/ai/Chapter7Note1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai7" src="https://www.pengyin-shan.com/images/2018/ai/Chapter7Note2.jpg"&gt;
&lt;/p&gt;&lt;hr&gt;
&lt;h2 id="chapter-8-application-of-deep-learning"&gt;Chapter 8: Application of Deep Learning&lt;/h2&gt;
&lt;p&gt;&lt;img alt="ai6" src="https://www.pengyin-shan.com/images/2018/ai/Chapter8Note1.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ai7" src="https://www.pengyin-shan.com/images/2018/ai/Chapter8Note2.jpg"&gt;
&lt;/p&gt;&lt;hr&gt;
&lt;h2 id="network-in-network-nin"&gt;Network in Network ï¼NiN)&lt;/h2&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>Machine Learning</category><category>Neural Network</category><category>Reading Notes</category><guid>https://www.pengyin-shan.com/posts/2018/ai/study-note-of-deep-learning-from-scratch/</guid><pubDate>Sun, 23 Dec 2018 05:00:00 GMT</pubDate></item><item><title>Collection of AI Business Cases - Marketing/Sale</title><link>https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/</link><dc:creator>Pengyin(Wendy) Shan</dc:creator><description>&lt;div&gt;&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#key-focus-for-companies"&gt;Key Focus for Companies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#marketing"&gt;Marketing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#agile-digital-marketing"&gt;Agile digital marketing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#optimizing-paid-search-sem"&gt;Optimizing paid search (SEM)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#seo"&gt;SEO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#personalization"&gt;Personalization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#sale"&gt;Sale&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#digital-buyingfulfillment"&gt;Digital buying/fulfillment&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#optimized-sales-coverage-models"&gt;Optimized sales-coverage models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#transforming-user-experience"&gt;Transforming User Experience&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#customer-decision-journeys-cdj"&gt;Customer decision journeys ï¼CDJ)&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#key-pain-points-in-cdj"&gt;Key Pain Points in CDJ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#solution"&gt;Solution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#optimizing-pricing"&gt;Optimizing Pricing&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#dynamic-deal-scoring-models"&gt;Dynamic deal-scoring models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#data-driven-performance-management"&gt;Data-driven performance management&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#iot"&gt;IoT&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#application"&gt;Application&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/#bibliography"&gt;Bibliography&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h2 id="key-focus-for-companies"&gt;Key Focus for Companies&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Understand more pain points of CDJ process&lt;/li&gt;
&lt;li&gt;One single source of selling data&lt;/li&gt;
&lt;li&gt;Link technology map to company value&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="marketing"&gt;Marketing&lt;/h2&gt;
&lt;h3 id="agile-digital-marketing"&gt;Agile digital marketing&lt;/h3&gt;
&lt;h3 id="optimizing-paid-search-sem"&gt;Optimizing paid search (SEM)&lt;/h3&gt;
&lt;h3 id="seo"&gt;SEO&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Maximize Organic Search&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="personalization"&gt;Personalization&lt;/h3&gt;
&lt;h2 id="sale"&gt;Sale&lt;/h2&gt;
&lt;h4 id="digital-buyingfulfillment"&gt;Digital buying/fulfillment&lt;/h4&gt;
&lt;h4 id="optimized-sales-coverage-models"&gt;Optimized sales-coverage models&lt;/h4&gt;
&lt;h3 id="transforming-user-experience"&gt;Transforming User Experience&lt;/h3&gt;
&lt;h4 id="customer-decision-journeys-cdj"&gt;Customer decision journeys ï¼CDJ)&lt;/h4&gt;
&lt;h5 id="key-pain-points-in-cdj"&gt;Key Pain Points in CDJ&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Discover Options: give a 'shopping list' to customer&lt;/li&gt;
&lt;li&gt;Quote: customers get quote for their desire, but not include everything or confused in 'shopping list'&lt;/li&gt;
&lt;li&gt;On-board: customer wait for quote approval, or need help from sales man first&lt;/li&gt;
&lt;li&gt;Purchase&lt;/li&gt;
&lt;li&gt;Receive Order: unclear order delay or no update of order status&lt;/li&gt;
&lt;li&gt;Manager: returning has to be done manually or inefficiently&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id="solution"&gt;Solution&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Build minimum viable product prototype&lt;/li&gt;
&lt;li&gt;Focus on the priories in key pain points of CDJ&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="optimizing-pricing"&gt;Optimizing Pricing&lt;/h3&gt;
&lt;h4 id="dynamic-deal-scoring-models"&gt;Dynamic deal-scoring models&lt;/h4&gt;
&lt;h4 id="data-driven-performance-management"&gt;Data-driven performance management&lt;/h4&gt;
&lt;h3 id="iot"&gt;IoT&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Industry: OEM, CPG&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id="application"&gt;Application&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Use senor to tailor offers and information to increase the propensity to buy&lt;/li&gt;
&lt;li&gt;Use leverage radio-frequency ID (&lt;code&gt;RFID&lt;/code&gt;) tags to automate inventory management&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="bibliography"&gt;Bibliography&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Why tech-enabled go-to-market innovation is critical for industrial companiesâand what to do about it, created by &lt;em&gt;Venkat Atluri&lt;/em&gt;ï¼&lt;em&gt;Satya Rao&lt;/em&gt; and &lt;em&gt;Andrew J. Wong&lt;/em&gt;,retrieved from https://www.mckinsey.com/business-functions/digital-mckinsey/our-insights/why-tech-enabled-go-to-market-innovation-is-critical-for-industrial-companies?cid=soc-app#0&lt;/li&gt;
&lt;/ul&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Marketing</category><category>Use Case</category><guid>https://www.pengyin-shan.com/posts/2018/ai/collection-of-ai-business-cases-marketing/</guid><pubDate>Fri, 30 Nov 2018 05:00:00 GMT</pubDate></item><item><title>Reading Notes of 'The Ai Frontier Insights from Hundreds of Use Cases' by McKinsey Global Institute</title><link>https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/</link><dc:creator>Pengyin(Wendy) Shan</dc:creator><description>&lt;div&gt;&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#overview-and-key-data"&gt;Overview and Key Data&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#data"&gt;Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#insights"&gt;Insights&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#neural-networks"&gt;Neural Networks&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#definitions"&gt;Definitions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#different-networks"&gt;Different Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#_1"&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#ffn"&gt;FFN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#rnn"&gt;RNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#cnn"&gt;CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#others"&gt;Others&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#gan"&gt;GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#reinforcement-learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#problem-types"&gt;Problem Types&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#classification"&gt;Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#continuous-estimation"&gt;Continuous Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#clustering"&gt;Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#all-other-optimizations"&gt;All other Optimizations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#anomaly-detection"&gt;Anomaly Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#ranking"&gt;Ranking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#recommendations"&gt;Recommendations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#data-generation"&gt;Data generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#predictive-maintenance"&gt;Predictive Maintenance&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#evaluation-of-ai"&gt;Evaluation of AI&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#which-industry"&gt;Which Industry?&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#personalize"&gt;Personalize&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#forecast"&gt;Forecast&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#greenfield-ai"&gt;Greenfield AI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#requirements-of-neutral-network-deployment"&gt;Requirements of Neutral Network Deployment&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#fitting-issue"&gt;Fitting Issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#collect-data"&gt;Collect Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#risks-mitigations"&gt;Risks &amp;amp; Mitigations&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#need-for-massive-data-sets"&gt;Need for massive data sets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#regulations"&gt;Regulations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#ways-of-adoption"&gt;Ways of Adoption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#culture-issue"&gt;Culture Issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#speed-issue"&gt;Speed Issue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#sell-buy-ai"&gt;Sell &amp;amp; Buy AI&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/#more-research"&gt;More Research&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;Following contents are retrieve from https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-applications-and-value-of-deep-learning&lt;/p&gt;
&lt;h2 id="overview-and-key-data"&gt;Overview and Key Data&lt;/h2&gt;
&lt;h3 id="data"&gt;Data&lt;/h3&gt;
&lt;p&gt;The deep learning techniques on which we focused â &lt;strong&gt;feed forward neural networks, recurrent neural networks, and convolutional neural networks&lt;/strong&gt;âaccount for about 40 percent of the &lt;em&gt;annual value&lt;/em&gt; potentially created by all analytics techniques. &lt;/p&gt;
&lt;p&gt;These three techniques together can potentially enable the creation of between $3.5 trillion and $5.8 trillion in value annually. Within industries, that is the equivalent of 1 to 9 percent of 2016 revenue.&lt;/p&gt;
&lt;p&gt;The AI techniques we focus on are &lt;em&gt;deep learning&lt;/em&gt; techniques based on artificial neural networks, which we see as generating as much as 40 percent of the &lt;strong&gt;total potential value&lt;/strong&gt; that all analytics techniques could provide.&lt;/p&gt;
&lt;p&gt;TWO-THIRDS OF THE OPPORTUNITIES TO USE AI ARE IN IMPROVING THE PERFORMANCE OF EXISTING ANALYTICS USE CASES&lt;/p&gt;
&lt;p&gt;In 69 percent of the use cases we studied, deep neural networks &lt;strong&gt;can be used to improve performance&lt;/strong&gt; beyond that provided by other analytic techniques. Cases in which only neural networks can be used, which we refer to here as &lt;code&gt;greenfield&lt;/code&gt; cases, constituted just 16 percent of the total. &lt;/p&gt;
&lt;p&gt;For the remaining 15 percent, artificial neural networks provided &lt;strong&gt;limited additional performance&lt;/strong&gt; over other analytics techniques, among other reasons because of data limitations that made these cases unsuitable for deep learning.&lt;/p&gt;
&lt;p&gt;Even the industry with the smallest potential value at stake, aerospace and defense (less than $50 billion) could enable the annual creation of value that is equivalent to the GDP of Lebanon.&lt;/p&gt;
&lt;p&gt;In our use cases, for example, we found that using &lt;strong&gt;real-time data&lt;/strong&gt; to predict hyper regional demand trends can increase sales by 0.25 percent to 0.75 percent, with margin improvements from lower waste and spoilage amounting to as much as half of one percentage point of sales. The impact can be considerably larger in pharmaceutical and medical products, in which predicting hyper-regional product demand and relevant health trends to inform inventory levels and reduce spoilage has the potential to raise sales by 5 to 10 percent.&lt;/p&gt;
&lt;h3 id="insights"&gt;Insights&lt;/h3&gt;
&lt;p&gt;Techniques that address &lt;strong&gt;classification, estimation, and clustering&lt;/strong&gt; problems are currently the most widely applicable in the use cases.&lt;/p&gt;
&lt;p&gt;The greatest potential for AI we have found is to create value in by established analytical techniques such as &lt;strong&gt;regression and classification &lt;/strong&gt;, but where neural network techniques could provide higher performance or generate additional insights and applications.&lt;/p&gt;
&lt;p&gt;The types of use cases with the greatest value potential vary by sector, and the &lt;strong&gt;effect&lt;/strong&gt; is also affected by the availability of data, its suitability for available techniques, and the applicability of various techniques and algorithmic solutions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Technical limitations&lt;/strong&gt; include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the need for a large volume and variety of often labeled training data&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the scale of the potential economic and societal impact creates an imperative for all the participantsâAI innovators, AI-using companies and policy-makers&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ultimately, the value of AI is not to be found in the models themselves, but in &lt;strong&gt;organizationsâ abilities to harness them&lt;/strong&gt;. Business leaders will need to &lt;em&gt;prioritize&lt;/em&gt; and make careful choices about how, when, and where to deploy them, also taking into account concerns including data security, privacy, and potential issues of bias.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="neural-networks"&gt;Neural Networks&lt;/h2&gt;
&lt;h3 id="definitions"&gt;Definitions&lt;/h3&gt;
&lt;p&gt;Neural networks are a subset of machine learning techniques. Essentially, they are AI systems based on simulating connected âneural units,â loosely modeling the way that neurons interact in the brain.&lt;/p&gt;
&lt;p&gt;AI practitioners refer to these techniques as âdeep learning,â since neural networks have many (âdeepâ) layers of simulated interconnected neurons. Before deep learning, neural networks often had only three to five layers and dozens of neurons; deep learning networks can have &lt;em&gt;seven to ten or more layers&lt;/em&gt;, with simulated neurons numbering into the millions.&lt;/p&gt;
&lt;h3 id="different-networks"&gt;Different Networks&lt;/h3&gt;
&lt;h3 id="_1"&gt;&lt;img alt="ai1" src="https://www.pengyin-shan.com/images/2018/ai/mcinsigh.png"&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img alt="ai2" src="https://www.pengyin-shan.com/images/2018/ai/mckensyinsight.jpg"&gt;&lt;/p&gt;
&lt;h4 id="ffn"&gt;FFN&lt;/h4&gt;
&lt;p&gt;Feed forward neural networks. One of the most common types of artificial neural network. In this architecture, information moves in only one direction, forward, from the input layer, through the âhiddenâ layers, to the output layer. There are no loops in the network.&lt;/p&gt;
&lt;p&gt;While the idea is not new, advances in computing power, training algorithms, and available data led to higher levels of performance than previously possible.&lt;/p&gt;
&lt;h4 id="rnn"&gt;RNN&lt;/h4&gt;
&lt;p&gt;Recurrent neural networks (RNNs). Artificial neural networks whose connections between neurons include loops, well-suited for processing sequences of inputs, which makes them highly effective in a wide range of applications, from handwriting, to texts, to speech recognition.&lt;/p&gt;
&lt;h4 id="cnn"&gt;CNN&lt;/h4&gt;
&lt;p&gt;Convolutional neural networks (CNNs). Artificial neural networks in which the connections between neural layers are inspired by the organization of the animal visual cortex, the portion of the brain that processes images, well suited for visual perception tasks.&lt;/p&gt;
&lt;h4 id="others"&gt;Others&lt;/h4&gt;
&lt;p&gt;We estimated the potential of those three deep neural network techniques to create value, as well as other machine learning techniques such as tree-based ensemble learning, classifiers, and clustering, and traditional analytics such as dimensionality reduction and regression.&lt;/p&gt;
&lt;h4 id="gan"&gt;GAN&lt;/h4&gt;
&lt;p&gt;Generative adversarial networks (GANs). These usually use two neural networks contesting each other in a zero-sum game framework (thus âadversarialâ). GANs can learn to mimic various distributions of data (for example text, speech, and images) and are therefore valuable in generating test datasets when these are not readily available. &lt;/p&gt;
&lt;h4 id="reinforcement-learning"&gt;Reinforcement Learning&lt;/h4&gt;
&lt;p&gt;Reinforcement learning. This is a subfield of machine learning in which systems are trained by receiving virtual ârewardsâ or âpunishments,â essentially learning by trial and error. Google DeepMind has used reinforcement learning to develop systems that can play games, including video games and board games such as Go, better than human champions.&lt;/p&gt;
&lt;h3 id="problem-types"&gt;Problem Types&lt;/h3&gt;
&lt;p&gt;In a business setting, those analytic techniques can be applied to solve real-life problems.&lt;/p&gt;
&lt;h4 id="classification"&gt;Classification&lt;/h4&gt;
&lt;p&gt;Based on a set of training data, categorize new inputs as belonging to one of a set of categories. An example of classification is identifying whether an image contains a specific type of object,&lt;/p&gt;
&lt;h4 id="continuous-estimation"&gt;Continuous Estimation&lt;/h4&gt;
&lt;p&gt;Based on a set of training data, estimate the next numeric value in a sequence. This type of problem is sometimes described as âprediction,â particularly when it is applied to time series data. One example of continuous estimation is forecasting the sales demand for a product.&lt;/p&gt;
&lt;h4 id="clustering"&gt;Clustering&lt;/h4&gt;
&lt;p&gt;These problems require a system to create a set of categories, for which individual data instances have a set of common or similar characteristics. An example of clustering is creating a set of consumer segments based on data&lt;/p&gt;
&lt;h4 id="all-other-optimizations"&gt;All other Optimizations&lt;/h4&gt;
&lt;p&gt;These problems require a system to generate a set of outputs that optimize outcomes for a specific objective function (some of the other problem types can be considered types of optimization, so we describe these as âall otherâ optimization). Generating a route for a vehicle that creates the optimum combination of time and fuel use is an example of optimization.&lt;/p&gt;
&lt;h4 id="anomaly-detection"&gt;Anomaly Detection&lt;/h4&gt;
&lt;p&gt;Given a training set of data, determine whether specific inputs are out of the ordinary.&lt;/p&gt;
&lt;h4 id="ranking"&gt;Ranking&lt;/h4&gt;
&lt;p&gt;Ranking algorithms are used most often in information retrieval problems in which the results of a query or request needs to be ordered by some criterion. Recommendation systems suggesting next product to buy use these types of algorithms as a final step, sorting suggestions by relevance, before presenting the results to the user.&lt;/p&gt;
&lt;h4 id="recommendations"&gt;Recommendations&lt;/h4&gt;
&lt;p&gt;These systems provide recommendations, based on a set of training data. A common example of recommendations are systems that suggest the ânext product to buyâ for a customer.&lt;/p&gt;
&lt;h4 id="data-generation"&gt;Data generation&lt;/h4&gt;
&lt;p&gt;These problems require a system to generate appropriately novel data based on training data.&lt;/p&gt;
&lt;h4 id="predictive-maintenance"&gt;Predictive Maintenance&lt;/h4&gt;
&lt;p&gt;The power of machine learning to detect anomalies. Some existing predictive maintenance systems have analyzed time series data from Internet of Things (IoT) sensors, such as those monitoring temperature or vibration, in order to detect anomalies or make forecasts on the remaining useful life of components. Deep learningâs capacity to analyze very large amounts of high dimensional data can take this to a new level. By layering in additional data, such as audio and image data, from other sensorsâincluding relatively cheap ones such as microphones and camerasâneural networks can enhance and possibly replace more traditional methods.&lt;/p&gt;
&lt;h2 id="evaluation-of-ai"&gt;Evaluation of AI&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Use case: the targeted application of digital technologies to a specific business challenge, with a measurable outcome.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;AI-driven logistics optimization can reduce costs through real-time forecasts and behavioral coaching. Application of AI techniques such as continuous estimation to logistics can add substantial value across many sectors. AI can optimize routing of delivery traffic, thereby improving fuel efficiency and reducing delivery times.&lt;/p&gt;
&lt;h3 id="which-industry"&gt;Which Industry?&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;THE BIGGEST VALUE OPPORTUNITIES FOR AI ARE IN MARKETING AND SALES AND IN SUPPLY-CHAIN MANAGEMENT AND MANUFACTURING&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;AI can be a valuable tool for &lt;strong&gt;customer service management and personalized marketing&lt;/strong&gt;. Improved speech recognition in call center management and call routing by applying AI techniques allow a more seamless experience for customersâand more efficient processing.&lt;/p&gt;
&lt;p&gt;Among business functions, these techniques are for now mostly to be found in marketing and sales and in supply-chain management and manufacturing. In particular, feed forward neural networks feature as the main technique deployed in these two functions.&lt;/p&gt;
&lt;p&gt;AIâs ability to conduct preventive maintenance and field force scheduling, as well as optimizing production and assembly processes, means that it also has considerable application possibilities and value potential across sectors including advanced electronics and semiconductors, automotive and assembly, chemicals, basic materials, transportation and logistics, oil and gas, pharmaceuticals and medical products, aerospace and defense, agriculture, and consumer packaged goods.&lt;/p&gt;
&lt;h4 id="personalize"&gt;Personalize&lt;/h4&gt;
&lt;p&gt;In retail, marketing and sales is the area with the most significant potential value from AI, and within that function, pricing and promotion and customer service management are the main value areas. Our use cases show that using customer data to personalize promotions, for example, including tailoring individual offers every day, can lead to a 1 to 2 percent increase in incremental sales for brick-and-mortar retailers alone.&lt;/p&gt;
&lt;h4 id="forecast"&gt;Forecast&lt;/h4&gt;
&lt;p&gt;Among the examples in our use cases, we see how forecasting based on underlying causal drivers of demand rather than prior outcomes can improve forecasting accuracy by 10 to 20 percent, which translates into a potential 5 percent reduction in inventory costs and revenue increases of 2 to 3 percent.&lt;/p&gt;
&lt;h4 id="greenfield-ai"&gt;Greenfield AI&lt;/h4&gt;
&lt;p&gt;Greenfield AI solutions are prevalent in business areas such as customer service management, as well as among some industries in which the data are rich and voluminous and at times integrate human reactions. &lt;/p&gt;
&lt;p&gt;A key differentiator that often underpins higher AI value potential is the possibility of &lt;em&gt;applying large amounts of audio, video, image, and text data&lt;/em&gt; to these problems. &lt;/p&gt;
&lt;p&gt;Among industries, we found many greenfield use cases in &lt;em&gt;health care&lt;/em&gt;, in particular. Some of these cases involve disease diagnosis and improved care, and rely on rich data sets incorporating image and video inputs, including from MRIs.&lt;/p&gt;
&lt;p&gt;In many of our use cases, however, traditional analytics and machine learning techniques continue to underpin a large percentage of the value creation potential in industries including insurance, pharmaceuticals and medical products, and telecommunications, with the potential of AI limited in certain contexts. &lt;/p&gt;
&lt;h3 id="requirements-of-neutral-network-deployment"&gt;Requirements of Neutral Network Deployment&lt;/h3&gt;
&lt;p&gt;Data volume is essential for neural networks to achieve a high level of accuracy in training algorithms.&lt;/p&gt;
&lt;p&gt;Most current AI models are trained through &lt;code&gt;supervised learning&lt;/code&gt;, which requires humans to label and categorize the underlying data. However promising new techniques are emerging to overcome these data bottlenecks, such as reinforcement learning, generative adversarial networks, transfer learning, and one-shot learning. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;One-shot learning&lt;/code&gt; allows a trained AI model to learn about a subject based on a small number of real world demonstrations or examplesâand sometimes just one.&lt;/p&gt;
&lt;h4 id="fitting-issue"&gt;Fitting Issue&lt;/h4&gt;
&lt;p&gt;Even with large datasets, they will have to guard against &lt;code&gt;over-fitting&lt;/code&gt; in which a model too tightly matches the ânoisyâ or random features of the training set, resulting in a corresponding lack of accuracy in future performance.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;under-fitting,&lt;/code&gt; in which the model fails to capture all of the relevant features. Linking data across customer segments and channels and, where possible, to production data, rather than allowing different sets of data to languish in silos, is especially important to create value.&lt;/p&gt;
&lt;p&gt;Realizing AIâs full potential requires a diverse range of data types including images, video, and audio Neural AI techniques excel at analyzing image, video, and audio data types because of their &lt;strong&gt;complex, multi-dimensional&lt;/strong&gt; nature, known by practitioners as &lt;code&gt;high dimensionality&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id="collect-data"&gt;Collect Data&lt;/h4&gt;
&lt;p&gt;Ongoing data acquisition for retraining AI systems is necessary; one out of three use cases requires model refreshes at least monthly and sometimes daily
highlight.&lt;/p&gt;
&lt;h2 id="risks-mitigations"&gt;Risks &amp;amp; Mitigations&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Prior research suggests that even among AI-aware firms, only about 20 percent are using one or more of the technologies in a core business process or at scale. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Limitations include the need for massive data sets, difficulties in explaining results, generalizing learning, and potential bias in data and algorithms&lt;/p&gt;
&lt;h3 id="need-for-massive-data-sets"&gt;Need for massive data sets&lt;/h3&gt;
&lt;p&gt;Promising new techniques are emerging to address this challenge, such as reinforcement learning (discussed earlier) and in-stream supervision, in which data can be labeled in the course of natural usage. &lt;/p&gt;
&lt;h3 id="regulations"&gt;Regulations&lt;/h3&gt;
&lt;p&gt;Among other constraints, regulators often want rules and choice criteria to be clearly explainable. Some nascent approaches to increasing model transparency, including local-interpretable-model-agnostic explanations (LIME), may help resolve this explanation challenge in many cases. &lt;/p&gt;
&lt;h3 id="ways-of-adoption"&gt;Ways of Adoption&lt;/h3&gt;
&lt;p&gt;Organizations planning to adopt significant deep learning efforts will need to consider a spectrum of options about how to do so. The range of options includes building a complete in-house AI capability either gradually in an organic way or more rapidly through acquisitions, outsourcing these capabilities, or leveraging AI-as-a-service offerings.&lt;/p&gt;
&lt;h3 id="culture-issue"&gt;Culture Issue&lt;/h3&gt;
&lt;p&gt;Process can also become an impediment to successful adoption unless organizations are digitally mature. On the technical side, organizations will have to develop robust data maintenance and governance processes, and implement modern software disciplines such as Agile and DevOps. &lt;/p&gt;
&lt;p&gt;Even more challenging, in terms of scale, is overcoming the âlast mileâ problem of making sure the superior insights provided by AI are instantiated in the behavior of the people and processes of an enterprise.&lt;/p&gt;
&lt;h3 id="speed-issue"&gt;Speed Issue&lt;/h3&gt;
&lt;p&gt;A recent Stanford University study found that deep neural networks can make highly accurate bond price predictions, but took hours to come up with the answer, whereas other âsimplerâ techniques produced an answer that was only slightly less accurate but very rapidâjust four seconds. 20 For a bond trader that timing difference is critical.&lt;/p&gt;
&lt;h3 id="sell-buy-ai"&gt;Sell &amp;amp; Buy AI&lt;/h3&gt;
&lt;p&gt;Many companies that develop or provide AI to others have considerable strength in the technology itself and the data scientists needed to make it work, but they can lack a deep understanding of end markets. This is a challenge, since our research has shown that most of the potential impact of AI comes from improving the performance in existing use casesâin other words, the fundamental drivers of the businesses of their potential customers. &lt;/p&gt;
&lt;p&gt;Furthermore, many of these companies are asking how they should prioritize their resources, whether it is R&amp;amp;D, marketing and sales, or other functions to take advantage of AI opportunities.&lt;/p&gt;
&lt;h4 id="more-research"&gt;More Research&lt;/h4&gt;
&lt;p&gt;Before launching more pilots or testing solutions, it is useful to step back and take a holistic approach to the issue, moving to create a prioritized portfolio of initiatives across the enterprise, including AI and the wider analytic and digital techniques available.&lt;/p&gt;&lt;/div&gt;</description><category>Machine Learning</category><category>Reading Notes</category><category>Use Case</category><guid>https://www.pengyin-shan.com/posts/2018/ai/reading-notes-the-ai-frontier-insights-from-hundreds-of-use-cases/</guid><pubDate>Tue, 27 Nov 2018 05:00:00 GMT</pubDate></item></channel></rss>